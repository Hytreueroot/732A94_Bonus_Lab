},
prob=NULL,
sort=NULL,
loop=NULL
)
library(Lab4)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(Lab4)
library(mlbench)
library(caret)
library(leaps)
set.seed(42)
data(BostonHousing)
indx <- createDataPartition(BostonHousing$tax, p = .80, list = FALSE, times = 1)
training_set <- BostonHousing[indx,]
test_set  <- BostonHousing[-indx,]
#Simple linear regression
fitted_lm = train(tax~., data=training_set, method="lm")
summary(fitted_lm)
#Linear regression with forward selection of covariates on the training dataset
fitted_lm_forward = train(tax~., data=training_set, method = "leapForward")
summary(fitted_lm_forward)
print(fitted_lm)
print(fitted_lm_forward)
develop_fitted_lm <- train(tax ~ zn + indus + rad + medv, data = training_set, method='lm')
summary(develop_fitted_lm)
print(fitted_lm)
print(fitted_lm_forward)
print(develop_fitted_lm)
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
data.frame(lambda = seq(0.1,5,0.1))
},
fit = function(x, y, wts, param, lev, last, weights, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, preProc=NULL, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=NULL,
loop=NULL
)
set.seed(42)
ridge_fit <- caret::train(tax~., data=training_set, method=ridge_model_info, tuneGrid=data.frame(lambda = seq(0.1,5,0.1)), trControl=trainControl(method = "repeatedcv", number=10, repeats=10))
ridge_fit$bestTune
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, df, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
ridge_fit$bestTune
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, df, submodels=NULL){
modelFit$predict(df)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, df, submodels=NULL){
modelFit$predict(df)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, df, submodels=NULL){
modelFit$predict(df)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(Lab4)
library(mlbench)
library(caret)
library(leaps)
set.seed(42)
data(BostonHousing)
indx <- createDataPartition(BostonHousing$tax, p = .80, list = FALSE, times = 1)
training_set <- BostonHousing[indx,]
test_set  <- BostonHousing[-indx,]
#Simple linear regression
fitted_lm = train(tax~., data=training_set, method="lm")
summary(fitted_lm)
#Linear regression with forward selection of covariates on the training dataset
fitted_lm_forward = train(tax~., data=training_set, method = "leapForward")
summary(fitted_lm_forward)
set.seed(42)
data(BostonHousing)
indx <- createDataPartition(BostonHousing$tax, p = .80, list = FALSE, times = 1)
training_set <- BostonHousing[indx,]
test_set  <- BostonHousing[-indx,]
#Simple linear regression
fitted_lm = train(tax~., data=training_set, method="lm")
summary(fitted_lm)
#Linear regression with forward selection of covariates on the training dataset
fitted_lm_forward = train(tax~., data=training_set, method = "leapForward")
summary(fitted_lm_forward)
print(fitted_lm)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(Lab4)
library(mlbench)
library(caret)
library(leaps)
set.seed(42)
data(BostonHousing)
indx <- createDataPartition(BostonHousing$tax, p = .80, list = FALSE, times = 1)
training_set <- BostonHousing[indx,]
test_set  <- BostonHousing[-indx,]
#Simple linear regression
fitted_lm = train(tax~., data=training_set, method="lm")
summary(fitted_lm)
#Linear regression with forward selection of covariates on the training dataset
fitted_lm_forward = train(tax~., data=training_set, method = "leapForward")
summary(fitted_lm_forward)
print(fitted_lm)
develop_fitted_lm <- train(tax ~ zn + indus + rad + medv, data = training_set, method='lm')
summary(develop_fitted_lm)
print(fitted_lm)
print(fitted_lm_forward)
print(develop_fitted_lm)
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(Lab4)
library(mlbench)
library(caret)
library(leaps)
set.seed(42)
data(BostonHousing)
indx <- createDataPartition(BostonHousing$tax, p = .80, list = FALSE, times = 1)
training_set <- BostonHousing[indx,]
test_set  <- BostonHousing[-indx,]
#Simple linear regression
fitted_lm = train(tax~., data=training_set, method="lm")
summary(fitted_lm)
#Linear regression with forward selection of covariates on the training dataset
fitted_lm_forward = train(tax~., data=training_set, method = "leapForward")
summary(fitted_lm_forward)
print(fitted_lm)
print(fitted_lm_forward)
develop_fitted_lm <- train(tax ~ zn + indus + rad + medv, data = training_set, method='lm')
summary(develop_fitted_lm)
print(fitted_lm)
print(fitted_lm_forward)
print(develop_fitted_lm)
ridge_model_info <- list(label= "Custom Ridge Regression Model",
library = "Lab4",
type = "Regression",
parameters = data.frame(parameter = "lambda",
class = "numeric",
label = "Lambda"),
grid = function(x, y, len=NULL, search = "grid"){
if (search == "grid") {
out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
}
else {
out <- data.frame(lambda = 10^runif(len, min = -5, 1))
}
out
},
fit = function(x, y, wts, param, lev, last, classProbs, ...){
Lab4::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels=NULL){
modelFit$predict(newdata)
},
prob=NULL,
sort=function(x){
x[order(-x$lambda),]
}
)
set.seed(42)
ridge_fit <- train(tax~., data=training_set, method=ridge_model_info, tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))), trControl=trainControl(method = "cv", number=10))
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
source("C:/Users/SK/Desktop/732A94_Bonus_Lab/R/ridgereg.R")
